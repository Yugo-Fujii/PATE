{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PATE-Practice2022130.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KDyppqYi5QW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86c3b1a8-ff94-4715-a39c-18b8489f9a8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 606 kB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 288 kB 46.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 41.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 37.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 62 kB 794 kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 42.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 25.5 MB 1.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 57 kB 5.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 961 kB 44.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 36.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 61 kB 399 kB/s \n",
            "\u001b[K     |████████████████████████████████| 789 kB 45.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 84 kB 2.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 42.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 39.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 35.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 40 kB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 251 kB 30.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 66 kB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 266 kB 45.0 MB/s \n",
            "\u001b[?25h  Building wheel for autodp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for forbiddenfruit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for names (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pymbolic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.4 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.4 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "flask 1.1.4 requires Werkzeug<2.0,>=0.15, but you have werkzeug 2.0.2 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "arviz 0.11.4 requires typing-extensions<4,>=3.7.4.3, but you have typing-extensions 4.0.0 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet syft"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# データのロード\n",
        "# Load the SVHN data set\n",
        "# SVHNからデータを読み込む\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "# 画像変換\n",
        "# 1.Totenser関数でテンソル化\n",
        "# 2.Normalise関数（平均、標準偏差）で前処理、torchvisionsの場合\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "# 訓練SVHN（数字）データセット（デイレクトリ、訓練用画像とラベル、画像をテンソル化し返す、なし、ダウンロード）\n",
        "train_data = datasets.SVHN('datasets/SVHN/train/', split='train', transform=transform,\n",
        "                                 target_transform=None, download=True)\n",
        "# テストSVHN\n",
        "test_data = datasets.SVHN('datasets/SVHN/test/', split='test', transform=transform,\n",
        "                               target_transform=None, download=True)\n",
        "\n",
        "# 教師の数\n",
        "num_teachers = 20  # -> 20に増やしてみる\n",
        "\n",
        "# 悪い教師の数\n",
        "num_malicious = 16 ## いろいろ試す   # 4, 8, 12 くらいで試してみる\n",
        "\n",
        "# 1バッチに含まれるデータ数\n",
        "batch_size = 64\n",
        "\n",
        "# get_data_loders関数（訓練用データセット、教師の数）\n",
        "def get_data_loaders(train_data, num_teachers):\n",
        "    # 教師分類器用のデータローダー作成関数 \n",
        "    \"\"\" Function to create data loaders for the Teacher classifier \"\"\"\n",
        "    # teacher_loders空リストの作成\n",
        "    teacher_loaders = []\n",
        "    # 教師あたりの訓練用データの総数知りたい\n",
        "    data_size = len(train_data) // num_teachers\n",
        "    print(\"教師の数\"+str(num_teachers))\n",
        "    print(\"悪い教師の数\"+str(num_malicious))\n",
        "    print(\"1教師あたりのデータサイズ\" + str(data_size))\n",
        "    print(\"バッチサイズ\"+str(batch_size))\n",
        "    print(\"サブセットの数\"+str(data_size / batch_size))\n",
        "    print(\"サブセットの数:1教師あたりのデータサイズ÷バッチサイズ:\" + str(data_size / batch_size))\n",
        "    # データ総数分くりかえす\n",
        "    for i in range(data_size):\n",
        "        # teacher ごとにデータを割り振る\n",
        "        indices = list(range(i*data_size, (i+1)*data_size))\n",
        "        subset_data = Subset(train_data, indices)\n",
        "        # ここでミニバッチのためのシャッフル\n",
        "        loader = torch.utils.data.DataLoader(subset_data, batch_size=batch_size, shuffle=True)\n",
        "        teacher_loaders.append(loader)\n",
        "    # 教師用分類器を戻す    \n",
        "    return teacher_loaders\n",
        "# teacher_loders教師用データ分類器の宣言\n",
        "teacher_loaders = get_data_loaders(train_data, num_teachers)"
      ],
      "metadata": {
        "id": "nbnG0hm6o82y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4955575b-d112-43ed-f7b2-792b2bb37e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: datasets/SVHN/train/train_32x32.mat\n",
            "Using downloaded and verified file: datasets/SVHN/test/test_32x32.mat\n",
            "教師の数20\n",
            "悪い教師の数16\n",
            "1教師あたりのデータサイズ3662\n",
            "バッチサイズ64\n",
            "サブセットの数57.21875\n",
            "サブセットの数:1教師あたりのデータサイズ÷バッチサイズ:57.21875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 各TeacherのCNNを構築\n",
        "# Define the teacher models and train them by defining a cnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# 最適化\n",
        "import torch.optim as optim\n",
        "\n",
        "# CNNsの構築\n",
        "# ネットワーク（classifier）をクラスとして作成、nn.Modulesを継承\n",
        "class Classifier(nn.Module):\n",
        "    # コンストラクタ\n",
        "    def __init__(self):\n",
        "        # nn.module内の_init_()を起動\n",
        "        super().__init__()\n",
        "        #self.は不変、convolutionやfully connectなどの学習に必要なパラメータを保持したいもの\n",
        "        # 1D畳み込み\n",
        "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n",
        "        # convolution (畳み込み)の定義（入力チャンネル数？、畳み込み後チャンネル数、正方形フィルタ） \n",
        "        # 2D畳み込み\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        # 2D特徴マップをランダムに0にする\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        #全結合層を定義する\n",
        "        #fc1の第一引数は、チャネル数*最後のプーリング層の出力のマップのサイズ=特徴量の数\n",
        "        # fully connectの定義（入力のサイズ（ベクトル）、出力後のベクトル）\n",
        "        self.fc1 = nn.Linear(5*10*10, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "    # 実際の処理、引数xがネットワークに対しての入力data、dataloaderから取得\n",
        "    def forward(self, x):\n",
        "        # 入力→畳み込み層1→活性化関数(ReLU)→プーリング層1(2*2)→出力\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        #入力→畳み込み層2→活性化関数(ReLU)→プーリング層2(2*2)→出力\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        #　\n",
        "        x = x.view(x.size(0), 5*10*10)\n",
        "        #入力→全結合層2→活性化関数(ReLU)→出力\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # モデルが訓練中か推論中か、training属性\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        #入力→全結合層2→出力\n",
        "        x = self.fc2(x)\n",
        "        # xが小さすぎて0を返すのを防ぐために対数とる\n",
        "        return F.log_softmax(x, dim=-1)\n",
        "\n",
        "class Classifier_small(nn.Module):\n",
        "    # コンストラクタ\n",
        "    def __init__(self):\n",
        "        # nn.module内の_init_()を起動\n",
        "        super().__init__()\n",
        "        #self.は不変、convolutionやfully connectなどの学習に必要なパラメータを保持したいもの\n",
        "        # 1D畳み込み\n",
        "        self.conv1 = nn.Conv2d(3, 1, kernel_size=5)\n",
        "        # convolution (畳み込み)の定義（入力チャンネル数？、畳み込み後チャンネル数、正方形フィルタ） \n",
        "        # 2D畳み込み\n",
        "        self.conv2 = nn.Conv2d(1, 20, kernel_size=5)\n",
        "        # 2D特徴マップをランダムに0にする\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        #全結合層を定義する\n",
        "        #fc1の第一引数は、チャネル数*最後のプーリング層の出力のマップのサイズ=特徴量の数\n",
        "        # fully connectの定義（入力のサイズ（ベクトル）、出力後のベクトル）\n",
        "        self.fc1 = nn.Linear(5*10*10, 5)\n",
        "        self.fc2 = nn.Linear(5, 10)\n",
        "    # 実際の処理、引数xがネットワークに対しての入力data、dataloaderから取得\n",
        "    def forward(self, x):\n",
        "        # 入力→畳み込み層1→活性化関数(ReLU)→プーリング層1(2*2)→出力\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        #入力→畳み込み層2→活性化関数(ReLU)→プーリング層2(2*2)→出力\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        #　\n",
        "        x = x.view(x.size(0), 5*10*10)\n",
        "        #入力→全結合層2→活性化関数(ReLU)→出力\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # モデルが訓練中か推論中か、training属性\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        #入力→全結合層2→出力\n",
        "        x = self.fc2(x)\n",
        "        # xが小さすぎて0を返すのを防ぐために対数とる\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "metadata": {
        "id": "XaVSHtLaqStG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 各Teacherが学習モデルを作る\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Defining the train and predict functions\n",
        "# 学習と予測関数\n",
        "# 学習の関数\n",
        "def train(model, trainloader, criterion, optimizer, epochs=10, is_malicious=False):\n",
        "    # 損失値の宣言\n",
        "    running_loss = 0\n",
        "    # エポック数だけ学習する \n",
        "    for e in tqdm(range(epochs)):\n",
        "        model.train()\n",
        "        # trainloaderからイテレートされたdataはimage, labelsという（配列？）なので分解      \n",
        "        for images, labels in trainloader:\n",
        "            images = images.to(\"cuda\")\n",
        "            labels = labels.to(\"cuda\")\n",
        "            if is_malicious:\n",
        "                images *= 0.1\n",
        "                # 悪い教師のラベルはランダムにする\n",
        "                labels = torch.randint(0, 10, (len(labels), )).to(\"cuda\")\n",
        "            # 勾配の初期化\n",
        "            optimizer.zero_grad()\n",
        "            # \n",
        "            output = model.forward(images)\n",
        "            # 損失関数の計算\n",
        "            loss = criterion(output, labels)\n",
        "            # 勾配の計算（誤差伝搬）\n",
        "            loss.backward()\n",
        "            # 重みの更新（最適化）\n",
        "            optimizer.step()\n",
        "            # 損失値\n",
        "            running_loss += loss.item()\n",
        "\n",
        "\n",
        "# モデルの訓練\n",
        "def train_models(num_teachers):\n",
        "    # モデルのリストを作る\n",
        "    models = []\n",
        "    # 教師の数だけループ\n",
        "    for i in range(num_teachers):\n",
        "        print(f\"{i}-th client training...\")\n",
        "        # classifier をつくる\n",
        "        model = Classifier().to(\"cuda\")\n",
        "        # 交差エントロピー（NNの出力をマイナス、重みの増加）\n",
        "        criterion = nn.NLLLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "        train(model, teacher_loaders[i], criterion, optimizer, is_malicious = (i < num_malicious))\n",
        "        models.append(model)\n",
        "    return models\n",
        "\n",
        "models = train_models(num_teachers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDvskTAFqXGV",
        "outputId": "7027de97-b8a3-46d5-8ab5-7a3d9ab74be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0-th client training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:13<00:00,  1.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-th client training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:13<00:00,  1.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2-th client training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:15<00:00,  1.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3-th client training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:13<00:00,  1.33s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4-th client training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:17<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5-th client training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:13<00:00,  1.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6-th client training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:13<00:00,  1.33s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7-th client training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:13<00:00,  1.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8-th client training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:13<00:00,  1.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9-th client training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:13<00:00,  1.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-th client training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:13<00:00,  1.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11-th client training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:13<00:00,  1.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12-th client training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:13<00:00,  1.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13-th client training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:13<00:00,  1.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14-th client training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:13<00:00,  1.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15-th client training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:13<00:00,  1.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16-th client training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:13<00:00,  1.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17-th client training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:13<00:00,  1.33s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18-th client training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:13<00:00,  1.33s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19-th client training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:13<00:00,  1.34s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Student用にデータをロードする\n",
        "student_train_data = Subset(test_data, list(range(9000)))\n",
        "student_test_data = Subset(test_data, list(range(9000, 10000)))\n",
        "\n",
        "student_train_loader = torch.utils.data.DataLoader(student_train_data, batch_size=batch_size)\n",
        "student_test_loader = torch.utils.data.DataLoader(student_test_data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "fCsV48hhM6PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの性能チェック\n",
        "images, labels = next(iter(teacher_loaders[0]))\n",
        "# images ... (num_samples x 3 x image_size x image_size)\n",
        "# models[0](images) ... (num_samples x 10)   10 .. クラス数（最終層がlog_softmaxなので）\n",
        "# models[0] ... 0番目のteacher model\n",
        "output = models[0](images.cuda())   #(num_samples x 10)\n",
        "# torch.argmax(value, dim=-1) ... -1の軸に対してargmaxをする。-1 ... 最後\n",
        "# ようは、10この数のなかで一番大きな数のindexを返す、をnum_samplesこやる\n",
        "print(torch.argmax(output, dim=-1))   # (num_samples, )\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ywiuR6sZyeV",
        "outputId": "9923154d-7b3b-460b-8940-eab91c5a8377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 8, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0')\n",
            "tensor([8, 5, 2, 1, 5, 3, 2, 6, 3, 1, 8, 9, 2, 1, 1, 7, 7, 3, 1, 1, 0, 4, 1, 2,\n",
            "        0, 5, 2, 2, 9, 1, 5, 1, 8, 7, 4, 0, 2, 3, 1, 5, 6, 1, 1, 8, 3, 5, 5, 1,\n",
            "        1, 6, 9, 1, 9, 2, 6, 4, 0, 8, 2, 4, 3, 3, 5, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next by combining the predictions of the Teacher models \n",
        "# we will generate the Aggregated Teacher and Student labels\n",
        "# 次に、教師モデルの予測を組み合わせることにより、\n",
        "# 集約された教師と生徒のラベルを生成します\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# predict関数（モデル、分類器）\n",
        "# 予測結果と正解データを比較すること\n",
        "def predict(model, dataloader):\n",
        "    # テンソルを返す、Dタイプのデータ型返す\n",
        "    outputs = torch.zeros(0, dtype=torch.long).to(\"cuda\")\n",
        "    # バッチ間の平均や分散を計算\n",
        "    model.eval()\n",
        "\n",
        "    # 分類器の中の画像とラベルのループ、詳細まだ\n",
        "    for images, labels in dataloader:  # batch_size 数のimage, labelをとってくる\n",
        "        images = images.to(\"cuda\")   # (batch_size x 3 x image_size x image_size)\n",
        "        labels = labels.to(\"cuda\")   \n",
        "        output = model.forward(images)  # (batch_size x 10)   ところでこのoutputはlog_softmax つまりsoftmaxの対数\n",
        "        ps = torch.argmax(torch.exp(output), dim=1)   # exp(log(softmax)) = softmax  1こめの軸（=10次元の軸）についてargmaxする  ... (batch_size, ) の出力\n",
        "        outputs = torch.cat((outputs, ps))\n",
        "\n",
        "\n",
        "    # もともと output = [] （なにもはいってない）\n",
        "    # 最初のforの中で、ps = [4,3,4] とかだったとする\n",
        "    # outputs = torch.cat(([], [4,3,4])) = [4,3,4]\n",
        "    # 次のループで [ 5,2,3]とかだったとする\n",
        "    # outputs = torch.cat(([4,3,4], [5,2,3])) = [4,3,4,5,2,3]\n",
        "    # ... というのを繰り返してくと、dataloaderに入っている画像全部についての、model.forwardの出力のargmaxがoutputに格納される\n",
        "        \n",
        "    return outputs\n",
        "\n",
        "epsilon = 0.5\n",
        "# 教師の統合関数（モデル、分類器、ε?何）\n",
        "def aggregated_teacher(models, dataloader, epsilon):\n",
        "    # テンソルを返す？\n",
        "    preds = torch.torch.zeros((len(models), 9000), dtype=torch.long)\n",
        "    # 要素のインデックスと要素\n",
        "    for i, model in enumerate(models):\n",
        "        results = predict(model, dataloader) # ... modelsから取り出された一つのmodelでの予測結果をresultsに格納している\n",
        "        preds[i] = results\n",
        "    \n",
        "    # \n",
        "    labels = np.array([]).astype(int)\n",
        "    for image_preds in np.transpose(preds):\n",
        "        label_counts = np.bincount(image_preds, minlength=10)\n",
        "        beta = 1 / epsilon\n",
        "\n",
        "        for i in range(len(label_counts)):\n",
        "            label_counts[i] += np.random.laplace(0, beta, 1)\n",
        "\n",
        "        new_label = np.argmax(label_counts)\n",
        "        labels = np.append(labels, new_label)\n",
        "    \n",
        "    return preds.numpy(), labels\n",
        "\n",
        "\n",
        "teacher_models = models\n",
        "# 関数の実行\n",
        "# 悪い教師も混ざっている場合\n",
        "preds, student_labels = aggregated_teacher(teacher_models, student_train_loader, epsilon)\n",
        "\n",
        "# いいteacherだけ使った場合（本来、誰が良いか、誰が悪いかわからないので、これはできない。誰が悪いかを見つける必要がある）\n",
        "\n",
        "# teacher_models[num_malicious:]  -> teacher_models[4] ... teacher_models[9]  で、いいteacherが合計9-4+1=6人\n",
        "# teacher_models[num_malicious:num_malicious * 2] =  teacher_models[4:4 * 2] =  teacher_models[4:8] = teacher[4] .. teacher[7] でいいteachcerが4人\n",
        "# good_teacher_models = teacher_models[num_malicious:] \n",
        "# print(len(good_teacher_models))\n",
        "# + teacher_models[num_malicious:num_malicious * 2]#ここのコードが分からなくなった \n",
        "# preds2, student_labels2 = aggregated_teacher(tm2, student_train_loader, epsilon)\n"
      ],
      "metadata": {
        "id": "dmfhfFRxs02B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_indices = np.arange(len(teacher_models))  # 全teacherのインデックス\n",
        "good_teacher_indices = teacher_indices[num_malicious:]  # 良いteacherのindex\n",
        "# np.random.choice(list, n) ... listの要素から重複を許してランダムにn要素を抽出\n",
        "\n",
        "good_teacher_models = []\n",
        "for i in range(len(teacher_models)):\n",
        "    idx = np.random.choice(good_teacher_indices, 1).item()\n",
        "    good_teacher_models.append(teacher_models[idx])\n",
        "preds2, student_labels2 = aggregated_teacher(good_teacher_models, student_train_loader, epsilon)"
      ],
      "metadata": {
        "id": "sZKxxA0VjCVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scipy ... 科学計算用のライブラリ / 特に、scipy.statsは統計計算（statistics）用のライブラリ\n",
        "import scipy.stats\n",
        "\n",
        "# scipy.stats.mode ... 列ごとに最頻値（mode）とその頻度を出力する関数\n",
        "mode = scipy.stats.mode(preds)\n",
        "mode_values = mode[0]  # (9000, 1)という形になっていて、実はちょっと使いにくい\n",
        "mode_values = mode_values.flatten()   # flatten ... (N, 1) -> (N, )という形に変換する\n",
        "mode_counts = mode[1] \n",
        "num_mismatches = np.zeros(len(teacher_models))  # teacher modelの数だけ0をの並べたベクトル\n",
        "\n",
        "for i in range(len(mode_values)):\n",
        "    mismatch = (preds[:, i] != mode_values[i]).astype(int)  # 最頻値と合致しない予測がある場合に1を返す\n",
        "    num_mismatches += mismatch  # それを加算する"
      ],
      "metadata": {
        "id": "bBHVbKL7fq6L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "0c138815-2a7f-458d-8ed7-a0569fecd206"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-65206b18e572>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# scipy.stats.mode ... 列ごとに最頻値（mode）とその頻度を出力する関数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmode_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (9000, 1)という形になっていて、実はちょっと使いにくい\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmode_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# flatten ... (N, 1) -> (N, )という形に変換する\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 最頻値から外れた確率を示している（教師分の配列）\n",
        "num_mismatches / len(mode_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3Vj5zhlgrmR",
        "outputId": "ecab4bc7-cd7d-41d5-c73f-865c0019303f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.87733333, 0.91366667, 0.87755556, 0.81855556, 0.84077778,\n",
              "       0.86655556, 0.88644444, 0.96077778, 0.91222222, 0.94633333,\n",
              "       0.85633333, 0.85444444, 0.866     , 0.90444444, 0.89844444,\n",
              "       0.90366667, 0.18877778, 0.18766667, 0.21522222, 0.18366667])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.bincount(preds[:, 2], minlength=10))  # 悪いの混ざってる\n",
        "print(np.bincount(preds2[:, 2], minlength=10))  # 混ざってない\n",
        "\n",
        "# この辺をちょっとデータ解析してみましょう。\n",
        "# 多数決で得られた結果と違う結果を出してしまった回数、をteacherごとに出してみるとどうか？ →（多数決といつも違うことをいっている = 間違っている可能性が高い）"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAec4ZJidyE2",
        "outputId": "d9ea30ae-447a-432b-b18f-472f4a1181d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3 6 1 2 1 1 3 1 1 1]\n",
            "[ 0 20  0  0  0  0  0  0  0  0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.bincount(preds[:, 8], minlength=10))  # 悪いの混ざってる\n",
        "print(np.bincount(preds2[:, 8], minlength=10))  # 混ざってない"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGJ7UB4GqqtQ",
        "outputId": "682866a6-614c-42c3-e9f7-0b0fadae82fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3 6 2 2 1 1 2 1 1 1]\n",
            "[ 0 20  0  0  0  0  0  0  0  0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5人より大勢が同じ答えを出す割合\n",
        "print((np.array([np.bincount(p, minlength=10).max() for p in preds.T]) > 5).sum() / len(preds.T))#5から10に変更\n",
        "\n",
        "# 正しいラベルを与える人を10人に水増ししたうえで、5人より大勢が同じ答えを出す割合\n",
        "print((np.array([np.bincount(p, minlength=10).max() for p in preds2.T]) > 5).sum() / len(preds2.T))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CPTLjgsfrwm",
        "outputId": "b8c9406c-4097-4bf6-e64d-a4e8cc652a71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3065555555555556\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now by using the labels generated previously we will create the Student model and train it\n",
        "# studentモデルの性能を調べる\n",
        "# Student_loaderから画像データ（X）を持ってきて、studentモデルの予測させ、とaggregated_teacherの正解データY（画像→ラベル）との精度を表示していく\n",
        "\n",
        "def student_loader(student_train_loader, labels):\n",
        "    for i, (data, _) in enumerate(iter(student_train_loader)):\n",
        "        yield data, torch.from_numpy(labels[i*len(data): (i+1)*len(data)])\n",
        "\n",
        "student_model = Classifier().to(\"cuda\")\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(student_model.parameters(), lr=0.003)\n",
        "#エポック数\n",
        "epochs = 20\n",
        "steps = 0\n",
        "running_loss = 0\n",
        "for e in tqdm(range(epochs)):\n",
        "    student_model.train()\n",
        "    train_loader = student_loader(student_train_loader, student_labels2)\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(\"cuda\")\n",
        "        labels = labels.to(\"cuda\")\n",
        "        steps += 1\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = student_model.forward(images)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        if steps % 50 == 0:\n",
        "            test_loss = 0\n",
        "            accuracy = 0\n",
        "            student_model.eval()\n",
        "            with torch.no_grad():\n",
        "                for images, labels in student_test_loader:\n",
        "                    images = images.to(\"cuda\")\n",
        "                    labels = labels.to(\"cuda\")\n",
        "                    log_ps = student_model(images)\n",
        "                    test_loss += criterion(log_ps, labels).item()\n",
        "                    \n",
        "                    ps = torch.exp(log_ps)\n",
        "                    top_p, top_class = ps.topk(1, dim=1)\n",
        "                    equals = top_class == labels.view(*top_class.shape)\n",
        "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "            student_model.train()\n",
        "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "                  \"Train Loss: {:.3f}.. \".format(running_loss/len(student_train_loader)),\n",
        "                  \"Test Loss: {:.3f}.. \".format(test_loss/len(student_test_loader)),\n",
        "                  \"Accuracy: {:.3f}\".format(accuracy/len(student_test_loader)))\n",
        "            running_loss = 0"
      ],
      "metadata": {
        "id": "m9n8NEuEbmIK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98526191-0b74-465e-f499-78499e8a07d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20..  Train Loss: 0.790..  Test Loss: 2.212..  Accuracy: 0.216\n",
            "Epoch: 1/20..  Train Loss: 0.771..  Test Loss: 2.116..  Accuracy: 0.299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 1/20 [00:03<01:15,  3.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2/20..  Train Loss: 0.730..  Test Loss: 1.914..  Accuracy: 0.354\n",
            "Epoch: 2/20..  Train Loss: 0.639..  Test Loss: 1.470..  Accuracy: 0.553\n",
            "Epoch: 2/20..  Train Loss: 0.533..  Test Loss: 1.230..  Accuracy: 0.628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 2/20 [00:08<01:14,  4.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3/20..  Train Loss: 0.504..  Test Loss: 1.117..  Accuracy: 0.675\n",
            "Epoch: 3/20..  Train Loss: 0.428..  Test Loss: 1.022..  Accuracy: 0.693\n",
            "Epoch: 3/20..  Train Loss: 0.406..  Test Loss: 0.986..  Accuracy: 0.684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 3/20 [00:12<01:11,  4.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4/20..  Train Loss: 0.469..  Test Loss: 0.973..  Accuracy: 0.693\n",
            "Epoch: 4/20..  Train Loss: 0.376..  Test Loss: 0.954..  Accuracy: 0.698\n",
            "Epoch: 4/20..  Train Loss: 0.374..  Test Loss: 0.902..  Accuracy: 0.717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 4/20 [00:16<01:07,  4.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5/20..  Train Loss: 0.449..  Test Loss: 0.906..  Accuracy: 0.711\n",
            "Epoch: 5/20..  Train Loss: 0.363..  Test Loss: 0.903..  Accuracy: 0.728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 5/20 [00:21<01:03,  4.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5/20..  Train Loss: 0.329..  Test Loss: 0.849..  Accuracy: 0.747\n",
            "Epoch: 6/20..  Train Loss: 0.409..  Test Loss: 0.877..  Accuracy: 0.737\n",
            "Epoch: 6/20..  Train Loss: 0.352..  Test Loss: 0.868..  Accuracy: 0.748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 6/20 [00:25<00:58,  4.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 7/20..  Train Loss: 0.352..  Test Loss: 0.921..  Accuracy: 0.716\n",
            "Epoch: 7/20..  Train Loss: 0.360..  Test Loss: 0.855..  Accuracy: 0.739\n",
            "Epoch: 7/20..  Train Loss: 0.338..  Test Loss: 0.895..  Accuracy: 0.717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▌      | 7/20 [00:29<00:54,  4.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 8/20..  Train Loss: 0.373..  Test Loss: 0.971..  Accuracy: 0.691\n",
            "Epoch: 8/20..  Train Loss: 0.329..  Test Loss: 0.867..  Accuracy: 0.737\n",
            "Epoch: 8/20..  Train Loss: 0.304..  Test Loss: 0.838..  Accuracy: 0.746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 8/20 [00:33<00:50,  4.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9/20..  Train Loss: 0.365..  Test Loss: 0.828..  Accuracy: 0.745\n",
            "Epoch: 9/20..  Train Loss: 0.307..  Test Loss: 0.846..  Accuracy: 0.735\n",
            "Epoch: 9/20..  Train Loss: 0.301..  Test Loss: 0.843..  Accuracy: 0.755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 9/20 [00:37<00:46,  4.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10/20..  Train Loss: 0.366..  Test Loss: 0.845..  Accuracy: 0.739\n",
            "Epoch: 10/20..  Train Loss: 0.302..  Test Loss: 0.776..  Accuracy: 0.760\n",
            "Epoch: 10/20..  Train Loss: 0.296..  Test Loss: 0.827..  Accuracy: 0.746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 10/20 [00:42<00:42,  4.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 11/20..  Train Loss: 0.366..  Test Loss: 0.834..  Accuracy: 0.759\n",
            "Epoch: 11/20..  Train Loss: 0.297..  Test Loss: 0.859..  Accuracy: 0.757\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 11/20 [00:46<00:38,  4.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 11/20..  Train Loss: 0.290..  Test Loss: 0.769..  Accuracy: 0.777\n",
            "Epoch: 12/20..  Train Loss: 0.365..  Test Loss: 0.795..  Accuracy: 0.762\n",
            "Epoch: 12/20..  Train Loss: 0.285..  Test Loss: 0.773..  Accuracy: 0.763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 12/20 [00:50<00:33,  4.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 13/20..  Train Loss: 0.323..  Test Loss: 0.820..  Accuracy: 0.762\n",
            "Epoch: 13/20..  Train Loss: 0.304..  Test Loss: 0.811..  Accuracy: 0.759\n",
            "Epoch: 13/20..  Train Loss: 0.285..  Test Loss: 0.800..  Accuracy: 0.756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 13/20 [00:54<00:29,  4.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 14/20..  Train Loss: 0.338..  Test Loss: 0.832..  Accuracy: 0.743\n",
            "Epoch: 14/20..  Train Loss: 0.297..  Test Loss: 0.786..  Accuracy: 0.767\n",
            "Epoch: 14/20..  Train Loss: 0.295..  Test Loss: 0.805..  Accuracy: 0.766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 14/20 [00:58<00:25,  4.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 15/20..  Train Loss: 0.351..  Test Loss: 0.797..  Accuracy: 0.764\n",
            "Epoch: 15/20..  Train Loss: 0.280..  Test Loss: 0.802..  Accuracy: 0.768\n",
            "Epoch: 15/20..  Train Loss: 0.264..  Test Loss: 0.831..  Accuracy: 0.764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 15/20 [01:03<00:20,  4.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 16/20..  Train Loss: 0.340..  Test Loss: 0.819..  Accuracy: 0.762\n",
            "Epoch: 16/20..  Train Loss: 0.288..  Test Loss: 0.770..  Accuracy: 0.761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 16/20 [01:07<00:16,  4.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 16/20..  Train Loss: 0.280..  Test Loss: 0.830..  Accuracy: 0.749\n",
            "Epoch: 17/20..  Train Loss: 0.323..  Test Loss: 0.785..  Accuracy: 0.769\n",
            "Epoch: 17/20..  Train Loss: 0.276..  Test Loss: 0.789..  Accuracy: 0.751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 17/20 [01:11<00:12,  4.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 18/20..  Train Loss: 0.316..  Test Loss: 0.834..  Accuracy: 0.736\n",
            "Epoch: 18/20..  Train Loss: 0.297..  Test Loss: 0.825..  Accuracy: 0.760\n",
            "Epoch: 18/20..  Train Loss: 0.273..  Test Loss: 0.766..  Accuracy: 0.772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 18/20 [01:15<00:08,  4.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 19/20..  Train Loss: 0.300..  Test Loss: 0.806..  Accuracy: 0.748\n",
            "Epoch: 19/20..  Train Loss: 0.287..  Test Loss: 0.852..  Accuracy: 0.741\n",
            "Epoch: 19/20..  Train Loss: 0.266..  Test Loss: 0.815..  Accuracy: 0.767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▌| 19/20 [01:19<00:04,  4.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 20/20..  Train Loss: 0.324..  Test Loss: 0.806..  Accuracy: 0.745\n",
            "Epoch: 20/20..  Train Loss: 0.278..  Test Loss: 0.800..  Accuracy: 0.767\n",
            "Epoch: 20/20..  Train Loss: 0.273..  Test Loss: 0.778..  Accuracy: 0.779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [01:23<00:00,  4.19s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we will perform PATE Analysis on the student labels generated \n",
        "# by the Aggregated Teacher\n",
        "from syft.frameworks.torch.dp import pate\n",
        "\n",
        "data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=preds, indices=student_labels, noise_eps=epsilon, delta=1e-5)\n",
        "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
        "print(\"Data Dependent Epsilon:\", data_dep_eps)"
      ],
      "metadata": {
        "id": "-TkWCmg5dV4N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "449e3f8e-ac72-450c-d29b-13370383e6ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-f2a6ec381631>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Now we will perform PATE Analysis on the student labels generated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# by the Aggregated Teacher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msyft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeworks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata_dep_eps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_ind_eps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher_preds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstudent_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_eps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'syft.frameworks'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-BcSOKPrQxJs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}